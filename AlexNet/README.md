# AlexNet 

## Paper 

This paper was particularly important to popularize deep learning and to push further the research interest in this field. 
The set of achievements described in the paper goes much beyond a simple performance improvement on the ImageNet dataset
and respective competition. The paper also proved that it is possible to go deeper, to build more complex models and 
increase the performance of previous models or architectures. To do that, for the first time, some new techniques were 
used together to improve the capability of the model. 

This is not only a nice paper with an even nicer architecture and results, but it also is a landmark in the history of 
deep learning.

### Introduction 

Until a few years before this paper, the size of the dataset was not considerably big, therefore some simple model 
conjugated with data augmentation techniques were enough to achieve a considerable performance and avoid overfitting. 
With the release of ImageNet dataset and others of the same kind, previous models became rather insufficient to learn 
all the complexities in the data distribution. Therefore other, more uncommon, models started to spark the interest of 
researchers like neural networks. 

However, using a neural network was not enough to achieve the desired performance with the new datasets. Neural networks 
were particularly challenging to train, with a substantial computational cost and prone to overfitting. Therefore the 
previous research papers on new activations like ReLU, regularization methods like Dropout and the presented 
implementation of efficient convolutions on a GPU were positively significant to the architecture described here. 